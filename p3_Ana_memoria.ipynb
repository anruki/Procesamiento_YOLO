{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anruki/Procesamiento_YOLO/blob/main/p3_Ana_memoria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PROCESAMIENTO DE VÍDEO PARA EL CUIDADO DE CABALLOS\n",
        "---\n",
        "### 🤓 **Autores**: Ana Robledano y Miguel Egido\n",
        "### 📸 **Asignatura**: Procesamiento Multimedia\n",
        "### 🧑 **Profesor**: Daniel Eusebio\n",
        "### 👀 **Algoritmo**: You Only Look Once (YOLO)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aqSlGPj6gU5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INTRODUCCIÓN\n",
        "\n",
        "En este proyecto se estudian casos de uso del procesamiento de vídeo junto con inteligencia artificial en el mundo ecuestre.\n",
        "\n",
        "Se realizan las siguientes técnicas:\n",
        "* __Preprocesamiento__: se aplican filtros, operaciones morfológicas,...\n",
        "* __Inteligencia Artificial__: YOLO para la detección de caballos\n",
        "* __Preprocesamiento__: para extraer características de los caballos detectados (su posición y velocidad).\n",
        "\n",
        "El preprocesamiento facilita la tarea de reconocimiento de la IA y reduce la capacidad de computación necesaria para la ejecución del algoritmo YOLO.\n",
        "\n",
        "El papel del procesamiento multimedia en este proyecto es crear un ``pipeline`` que transforma el vídeo input para que pueda ser procesado por el algoritmo YOLO.\n",
        "\n"
      ],
      "metadata": {
        "id": "wthxeZYrXhh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1. Preprocesamiento"
      ],
      "metadata": {
        "id": "uKzO1w5rbKhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kdT9Br6_cw06"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hz6hIBGmcxE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2.Algoritmo You Only Look One (YOLO)\n"
      ],
      "metadata": {
        "id": "hnPNA7X1bOrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo You Only Look Once (YOLO), es un sistema de código abierto para detección de objetos en tiempo real, el cual hace uso de una única red neuronal convolucional para detectar objetos en imágenes. Para su funcionamiento, la red neuronal divide la imagen en regiones, prediciendo cuadros de identificación y probabilidades por cada región; las cajas son ponderadas a partir de las probabilidades predichas. El algoritmo aprende representaciones generalizables de los objetos, permitiendo un bajo error de detección para entradas nuevas, diferentes al conjunto de datos de entrenamiento.1\n",
        "\n",
        "El algoritmo base corre a 45 cuadros por segundo (FPS) sin procesamiento de lote en un GPU Titan X. Debido a sus características de procesamiento, el algoritmo es utilizado en aplicaciones de detección de objetos en transmisión de video con retazo de señal menor a 25 milisegundos.2\n",
        "\n",
        "Arquitectura\n",
        "El modelo se implementó como una red neuronal convolucional y fue evaluado en el set de datos para detección de PASCAL VOC. Las capas convolucionales iniciales de la red se encargan de la extracción de características de la imagen, mientras que las capas de conexión completa predicen la probabilidad de salida y las coordenadas del objeto.\n",
        "\n",
        "La red tiene 24 capas convolucionales seguidas por 2 capas de conexión completa; esta hace uso de capas de reducción de 1x1 seguidas capas convolucionales de 3x3. El modelo Fast YOLO hace uso de una red neuronal de 9 capas. La salida final del modelo tensor de predicción de 7x7x30.3​\n",
        "\n",
        "\n",
        "Limitaciones\n",
        "El algoritmo delimita fuertes restricciones espaciales en los límites de la caja de predicción dado que cada celda predice únicamente dos cajas y una clase; esto limita el número de objetos que se pueden detectar, lo cual hace que el algoritmo se vea limitado en la detección de objetos presentados en grupos.\n",
        "\n",
        "Presenta problemas en la generalización de objetos visualizados en distintas configuraciones, al igual que en el proceso de post procesamiento de errores, tratando todos los errores con el mismo nivel de importancia.4​"
      ],
      "metadata": {
        "id": "BpBpMiFBcRln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python ultralytics"
      ],
      "metadata": {
        "id": "otPAK63mgYad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfeb9153-1860-426f-cd7b-dea863aa7780"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.55-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.55-py3-none-any.whl (904 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.3/904.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.55 ultralytics-thop-2.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "t6crU69DcZen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280f1ca2-2d65-4fc5-c266-1de0060160ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Por qué Inteligencia Artificial?\n",
        "Aunque el uso de Inteligencia Artificial supone un coste adicional en el procesamiento multimedia, aporta mayor flexibilidad de escenarios.\n",
        "\n",
        "La detección de caballos podría realizarse manualmente mediante funciones de clusterización que no utilizan IA. No obstante, este proyecto está enfocado en el cuidado de distintos tipos y tamaños de caballos y otros animales. El modelo YOLO preentrenado permite la clasificación efectiva de distintas clases de animales independientemente de un área específica.\n",
        "\n",
        "Concretamente para este proyecto nos centraremos en el procesamiento de vídeo para el cuidado de caballos. Con la colaboración de Hípica Natur http://hipicamadrid.es/ que nos ha facilitado la recogida de vídeos en un entorno ganadero real."
      ],
      "metadata": {
        "id": "QzMiUIB7ePwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Aplicación real\n",
        "\n",
        "A continuación se muestra el procesamiento de vídeo de un caballo para su detección mediante los apartados definidos anteriormente."
      ],
      "metadata": {
        "id": "lhBoZz4nbbgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importación de librerías y del vídeo"
      ],
      "metadata": {
        "id": "ZyxfP1Zsgbju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "ARkUQsnwgb7k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = 'sunday_vid.mov'\n",
        "cap = cv2.VideoCapture(video_path)"
      ],
      "metadata": {
        "id": "IiyN8-ZAdLWW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga del modelo `` YOLO V8 ``\n",
        "\n"
      ],
      "metadata": {
        "id": "ulMdrkCcdPIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load yolov8 model\n",
        "model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "id": "ZCx9fmcxiMts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79907fae-6634-41e2-ce09-55dec2a13455"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 53.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procesamiento del vídeo\n",
        "Mediante un bucle que recorre cada frame:"
      ],
      "metadata": {
        "id": "0BvU-rJVdbFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if video loaded successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video or cannot fetch the frame.\")\n",
        "        break\n",
        "\n",
        "    # Detect and track objects\n",
        "    results = model.track(frame, persist=True)\n",
        "\n",
        "    # Plot results\n",
        "    frame_ = results[0].plot()  # Annotated frame\n",
        "\n",
        "    # Display the annotated frame\n",
        "    cv2.imshow('YOLOv8 Tracking', frame_)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print('Video Processing Complete.')"
      ],
      "metadata": {
        "id": "H6lLVxvMdh-S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación del código** (..)"
      ],
      "metadata": {
        "id": "hxhHk8aRNwsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para un vídeo con la cámara no estática y donde el caballo se acerca y aleja del plano, el algoritmo YOLO detecta con una precisión alta al caballo.\n",
        "\n",
        "\n",
        "El paso siguiente será utilizar la posición del caballo para determinar si se escapa del cercado o no. El objetivo es crear un sistema que alerte cuando algún caballo escape del cercado, teniendo en cuenta que cuando lo saca una persona el sistema no debe activar ninguna alerta."
      ],
      "metadata": {
        "id": "21jzgmwyzEro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. POSICIÓN - Sistema de alerta cuando un caballo escapa"
      ],
      "metadata": {
        "id": "wd1vorA32bl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las mayores preocupaciones de los ganaderos es el bienestar de sus animales. Aunque las instalaciones estén pensadas para el ganado, puede haber problemas técnicos (como fallas en el sistema de cercado eléctrico) u otras situaciones impredecibles. https://www.youtube.com/watch?v=K1FPRo7hO7I\n",
        "\n",
        "Las consecuencias de que un animal se escape pueden ser fatales ya que si huyen hacia la carretera pueden provocar accidentes graves.\n",
        "\n",
        "Este proyecto busca automatizar la vigilancia, permitiendo a los ganaderos dedicarse a otras tareas o a descansar mientras el sistema vigila y alerta en caso de que un animal escape."
      ],
      "metadata": {
        "id": "ZkaGhM9E9ahk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ello, se siguen los pasos anteriores (preprocesamiento y YOLO), añadiendo una ``región de interés`` de la que si el caballo sale, se cambiará el color de su ``bounding box`` y se activará una alerta.\n",
        "\n",
        "* La región de interés queda delimitada por una valla, cuya posición dependerá del plano de la cámara y la posición de la valla. Este paso no es automatizable, pero es sencillo de aplicar a cualquier escenario similar. - ``posibilidad de hacer interactivo``.\n",
        "* En un cercado puede haber más de un caballo, a cada caballo se le asignará una etiqueta distinta, cuando un caballo escape se hará una captura de pantalla del caballo que escapó para que el dueño pueda identificarlo rapidamente y se llevará un recuento del nº de caballos que escaparon. ``vista a futuro: posibilidad de hacer interactivo haciendo una etiqueta personalizada para cada caballo con su nombre``.\n",
        "* Cuando se identifique a una persona (por el algoritmo YOLO), no se activará ninguna alerta, ya que esto sugiere que la persona intencionalmente sacó al caballo del cercado. Esta función podría desactivarse para que siempre saltase la alarma (aunque haya personas) y evitar robos. - ``posibilidad de hacer interactivo``."
      ],
      "metadata": {
        "id": "hH-Z-4r74PQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo YOLOv8\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Ruta del video de entrada\n",
        "video_path = 'caballo.mp4'\n",
        "\n",
        "# Cargar el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# CREAR VIDEO PROCESADO (OUTPUT)\n",
        "#------------------------------------------------------------------\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_path = 'caballo_procesado_caballos.mp4'\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "ret = True\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "# PROCESAMIENTO\n",
        "while ret:\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        # TRACKLING\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # DETECTAR SOLO CABALLOS\n",
        "        for result in results[0].boxes:\n",
        "            label = model.names[int(result.cls)]\n",
        "            if label == 'horse':\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "\n",
        "                # PINTAR DE COLOR VERDE SI EL CABALLO SE ENCUENTRA DENTRO DEL RECINTO Y DE ROJO SI ESTA FUERA\n",
        "                center_x = (x1 + x2) // 2\n",
        "                color = (0, 255, 0) if center_x < width // 2 else (0, 0, 255)\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J4z8v2nAgv_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación del código** (..)"
      ],
      "metadata": {
        "id": "_a-IZzJuN4Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso `` width `` mide el ancho del video en píxeles, y se usa para determinar si el caballo detectado está a la izquierda o a la derecha del video. Para poder generalizar el programa a cualquier espacio, se podría recomendar colocar la cámara de manera que la valla quede en la mitad del frame.\n",
        "\n",
        "No obstante, si la cámara tiene una posición en la que la valla no queda en el medio se puede ajustar mediante un punto de Corte Manual.\n",
        "\n",
        "__Pseudocódigo:__\n",
        "```\n",
        "si centro < x = 800 -> pintar de rojo\n",
        "sino -> pintar de verde\n",
        "```\n",
        "Es decir, pasados 800 píxeles desde el borde izquierdo, se utiliza el color verde.\n",
        "\n",
        "__Código:__\n",
        "```\n",
        "cutoff = 800\n",
        "color = (0, 0, 255) if center_x < cutoff else (0, 255, 0)\n",
        "```\n",
        "Para otro plano de la cámara se deberá ajustar el nº de píxeles para que coincida con la valla.\n"
      ],
      "metadata": {
        "id": "eXxIG9gnH7d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta del video de entrada\n",
        "video_path = 'caballos.mp4'\n",
        "\n",
        "# Cargar el video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Obtener propiedades del video\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Verificar si el video se cargó correctamente\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: No se pudo abrir el video.\")\n",
        "    exit()\n",
        "\n",
        "# Procesamiento del video\n",
        "while True:\n",
        "    # Leer un frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Fin del video o error al cargar el frame.\")\n",
        "        break\n",
        "\n",
        "    # Tracking con YOLOv8\n",
        "    results = model.track(frame, persist=True)\n",
        "\n",
        "    # Procesar detecciones\n",
        "    for result in results[0].boxes:  # Acceder a las cajas detectadas\n",
        "        label = model.names[int(result.cls)]  # Obtener el nombre de la clase\n",
        "        if label == 'horse':  # Filtrar solo caballos\n",
        "            # Coordenadas del cuadro delimitador\n",
        "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "\n",
        "            # Determinar el color del cuadro según la posición\n",
        "            center_x = (x1 + x2) // 2\n",
        "\n",
        "            # AJUSTE MANUAL DE LA DELIMITACIÓN DE LA VALLA\n",
        "            # --------------------------------------------\n",
        "            cutoff = 800  # 900 píxeles desde el borde izquierdo\n",
        "            color = (0, 0, 255) if center_x < cutoff else (0, 255, 0)\n",
        "\n",
        "            # Dibujar el cuadro y la etiqueta\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Mostrar el frame procesado\n",
        "    cv2.imshow('Video Procesado', frame)\n",
        "\n",
        "    # Salir con la tecla 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Liberar recursos\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print('Procesamiento de video completado.')\n"
      ],
      "metadata": {
        "id": "EsPY_UIGIY70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez controlada la posición del caballo, podemos también deducir su velocidad."
      ],
      "metadata": {
        "id": "GYU0AhVOLQBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. VELOCIDAD - Cálculo de la velocidad de un caballo\n"
      ],
      "metadata": {
        "id": "NW-ZZHIALdTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ámbito de las carreras o estudios biológicos de velocidades de cada raza de caballo es muy útil detectar la velocidad de un caballo dado un vídeo.\n",
        "\n",
        "Para ello, utilizaremos el mismo algoritmo YOLO que calcula la posición del caballo. Puesto que la velocidad es la derivada de la posición: ....\n",
        "\n",
        "\n",
        "* Se calcula la velocidad aproximada. Se debe introducir al programa la altura desde la cruz hasta el suelo del caballo. A partir del tamaño del caballo, se utilizará esa medida para deducir la distancia que recorre.\n",
        "\n",
        "El tamaño de un caballo suele ser facilmente accesible para su dueño ya que está especificado en el pasaporte obligatorio del animal, de otro modo se podría medir el caballo manualmente y anotarlo."
      ],
      "metadata": {
        "id": "jJO8yhRKLnqS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLyjpzs1Mt8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación del código** (..)"
      ],
      "metadata": {
        "id": "p1QKuWjSN8ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. RESULTADOS\n",
        "A continuación se prueban los programas con varios ejemplos para verificar su fiabilidad y precisión."
      ],
      "metadata": {
        "id": "fGHkvqoLMuxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. CONCLUSIONES"
      ],
      "metadata": {
        "id": "6muNMDJANn3_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}